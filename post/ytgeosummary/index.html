<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.81.0"><title>A quick primer on volume rendering of geophysics data with yt &#183; Chris Havlin</title><meta name=description content><meta itemprop=name content="A quick primer on volume rendering of geophysics data with yt"><meta itemprop=description content="Over the past couple years I&rsquo;ve worked on volume rendering of 3D geoscience data using yt in a number of ways."><meta itemprop=datePublished content="2022-01-21T13:55:49-06:00"><meta itemprop=dateModified content="2022-01-21T13:55:49-06:00"><meta itemprop=wordCount content="2327"><meta itemprop=image content="https://chrishavlin.github.io/images/turt.png"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chrishavlin.github.io/images/turt.png"><meta name=twitter:title content="A quick primer on volume rendering of geophysics data with yt"><meta name=twitter:description content="Over the past couple years I&rsquo;ve worked on volume rendering of 3D geoscience data using yt in a number of ways."><meta property="og:title" content="A quick primer on volume rendering of geophysics data with yt"><meta property="og:description" content="Over the past couple years I&rsquo;ve worked on volume rendering of 3D geoscience data using yt in a number of ways."><meta property="og:type" content="article"><meta property="og:url" content="https://chrishavlin.github.io/post/ytgeosummary/"><meta property="og:image" content="https://chrishavlin.github.io/images/turt.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-01-21T13:55:49-06:00"><meta property="article:modified_time" content="2022-01-21T13:55:49-06:00"><meta property="og:site_name" content="Chris Havlin"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://chrishavlin.github.io/#author","name":null,"image":{"@type":"ImageObject","url":"https://chrishavlin.github.io/images/turt.png"},"description":" "},{"@type":"WebSite","@id":"https://chrishavlin.github.io/#website","url":"https://chrishavlin.github.io/","name":"Chris Havlin","description":" ","publisher":{"@id":"https://chrishavlin.github.io/#author"},"inLanguage":"en"},{"@type":"ImageObject","url":"https://chrishavlin.github.io/images/turt.png","caption":"Chris Havlin"},{"@type":"WebPage","@id":"https://chrishavlin.github.io/post/ytgeosummary/#webpage","url":"https://chrishavlin.github.io/post/ytgeosummary/","name":"A quick primer on volume rendering of geophysics data with yt","isPartOf":{"@id":"https://chrishavlin.github.io/#website"},"about":{"@id":"https://chrishavlin.github.io/#author"},"datePublished":"2022-01-21T13:55:49-06:00","dateModified":"2022-01-21T13:55:49-06:00","description":"Over the past couple years I\u0026rsquo;ve worked on volume rendering of 3D geoscience data using yt in a number of ways.","inLanguage":"en","potentialAction":[{"@type":"ReadAction","target":["https://chrishavlin.github.io/post/ytgeosummary/"]}]},{"@type":"Article","isPartOf":{"@id":"https://chrishavlin.github.io/post/ytgeosummary/#webpage"},"mainEntityOfPage":{"@id":"https://chrishavlin.github.io/post/ytgeosummary/#webpage"},"headline":"A quick primer on volume rendering of geophysics data with yt","datePublished":"2022-01-21T13:55:49-06:00","dateModified":"2022-01-21T13:55:49-06:00","publisher":{"@id":"https://chrishavlin.github.io/#author"},"keywords":[],"articleSection":[],"inLanguage":"en","author":{"@type":"Person","name":null},"potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https://chrishavlin.github.io/post/ytgeosummary/#comments"]}]}]}</script><link type=text/css rel=stylesheet href=/css/print.css media=print><link type=text/css rel=stylesheet href=/css/poole.css><link type=text/css rel=stylesheet href=/css/hyde.css><style type=text/css>.sidebar{background-color:#3b7044}.read-more-link a{border-color:#3b7044}.read-more-link a:hover{background-color:#3b7044}.pagination li a{color:#3b7044;border:1px solid #3b7044}.pagination li.active a{background-color:#3b7044}.pagination li a:hover{background-color:#3b7044;opacity:.75}footer a,.content a,.related-posts li a:hover{color:#3b7044}</style><link type=text/css rel=stylesheet href=/css/blog.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin=anonymous><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png></head><body><aside class=sidebar><div class=container><div class=sidebar-about><div class=author-image><a href=https://chrishavlin.github.io/><img src=/images/turt.png class="img-circle img-headshot center" alt="Profile Picture"></a></div><h1>Chris Havlin</h1><p class=lead></p></div><nav><ul class=sidebar-nav><li><a href=https://chrishavlin.github.io/>Home</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Posts</a></li><li><a href=/categories/>Categories</a></li><li><a href=/tags/>Tags</a></li></ul></nav><section class=social-icons><a href=https://github.com/chrishavlin rel=me title=GitHub target=_blank><i class="fab fa-github" aria-hidden=true></i></a><a href=https://twitter.com/s_i_r_h_c rel=me title=@s_i_r_h_c target=_blank><i class="fab fa-twitter" aria-hidden=true></i></a><a href=https://www.instagram.com/chrishavlin/ rel=me title=@chrishavlin target=_blank><i class="fab fa-instagram" aria-hidden=true></i></a><a href=https://www.linkedin.com/in/christopherhavlin/ rel=me title=Linkedin target=_blank><i class="fab fa-linkedin" aria-hidden=true></i></a></section></div></aside><main class="content container"><div class=post><h1 class=title>A quick primer on volume rendering of geophysics data with yt</h1><div class=post-date><time datetime=2022-01-21T13:55:49-0600>Jan 21, 2022</time> <span class=readtime>&#183; 11 min read</span></div><div><p>Over the past couple years I&rsquo;ve worked on volume rendering of 3D geoscience data using yt in a number of ways. This post serves as quick primer on that work, with suggestions for how to get started and aspects that may be tricky!</p><h2 id=volume-rendering>Volume rendering?</h2><p>When working with 3D volumetric data, scientists usually rely on visualizations that reduce the dimensionality of the data. This reduction could simply be a sampling like a 2D slice through a 3D volume or it could involve some operation like an integration that collapses one of the dimensions. It makes sense that we rely on lower dimensionality plots &ndash; ultimately we&rsquo;re limited by our 2d media and until everyone has immersive 3D virtual environments we have to find a way to put our 3D data onto a 2D plane whether it&rsquo;s paper or screen. But what if we could easily create renderings on a 2d page that inherently capture the 3d spatial relationships we&rsquo;re interested in? Enter volume rendering!</p><p>Put <strong>very</strong> simply, volume rendering is a way of sampling a 3D volume by projecting parallel rays through the volume and aggregating data along each ray. But if we aggregate the data in the right way, we can actually retain information on spatial relationships! This way of projecting rays is derived from <a href=https://en.wikipedia.org/wiki/Ray_tracing_(graphics)#Algorithm_overview>ray tracing in computer graphics</a> and as such, a lot of the terminology and concepts tend to have computer graphiscy names (which will be unfamiliar to most physical scientists). For a very nice overview that doesn&rsquo;t get too technical, check out <a href=https://ieeexplore.ieee.org/abstract/document/4418774>Callahan et. al, 2008</a>.</p><p>So of the two parts of volume rendering, ray projection and aggregation along rays, it is typically the aggregation that we have to think the most about. Since we&rsquo;re not talking about optical ray tracing (where the rays are actual light rays that may bend and bounce), we don&rsquo;t generally have to worry too much about the details of how the ray tracing works. The aggregation, however, is what controls our final image, and it is controlled by a <strong>transfer function</strong>. Let&rsquo;s take a look at a rendering to explain it! The following is an example of a 3D volume rendering from <a href=https://www.essoar.org/doi/abs/10.1002/essoar.10506118.2>Havlin et al. 2020</a> using the <a href=https://yt-project.org>yt package</a>:</p><p><img src=/images/volrenderprimer/ecube_vr_w_tf.png alt=png></p><p>The right panel is our rendering and the left panel is the transfer function. The transfer function has two adjustable parameters: a transmission coefficient, <code>alpha</code>, and a color mapping. Here, we can see 2 gaussians (red and blue) centered at a narrow range of positive and negative velocity anomalies. At each step along each ray, the data is sampled, and then the transfer function is used to look up the color value and transmission coefficient for that data sample and then the color is blended with the cumulative value along the ray based on the transmission coefficient. The resulting image clearly preserves spatial relationships (the red fields are exterior to the blues) and we have a record of how the image was made (the transfer function).</p><p>In yt, you have <strong>direct</strong> control over the transfer function used. This is both awesome and challenging. It means that you will be able to check the sensitivity of your images to different transfer functions (awesome) and share your tailor-made transfer functions with others via code snippets (awesome) but it also means you have to figure out what to use for a transfer function (challenging).</p><h2 id=getting-started>Getting started</h2><p>In this primer, I mostly present my general workflow to help you get started. While there are some code snippets, they are meant more to describe the process in general. For more fully worked examples you can check out the links below:</p><ul><li>Havlin et. al, EarthCube 2020 (<a href=https://www.essoar.org/doi/abs/10.1002/essoar.10506118.2>summary</a>, <a href=https://github.com/earthcube2020/ec20_havlin_etal>repository link</a>, <a href=https://nbviewer.org/github/earthcube2020/ec20_havlin_etal/blob/master/notebook/ec20_havlin_etal.ipynb>static notebook rendering</a>): a self-contained notebook from the EarthCube 2020 annual meeting demonstrating volume rendering of a seismic tomography model of the upper mantle in the western U.S.</li><li>Havlin et. al, AGU 2020 (<a href=github.com/chrishavlin/AGU2020>repo link</a>): an AGU &ldquo;poster&rdquo; full of notebooks demonstrating how to use yt with a range of geoscience data .</li></ul><p>Additionally, I&rsquo;ll refer to a number of repositories:</p><ul><li>yt: <a href=https://yt-project.org/>https://yt-project.org/</a> Even if you&rsquo;re unfamiliar with astro datasets, it&rsquo;s worth first working through the <a href=https://yt-project.org/doc/quickstart/index.html>quick start</a> and then reading the <a href=https://yt-project.org/doc/visualizing/volume_rendering.html>overview on volume rendering</a></li><li><a href=https://github.com/chrishavlin/ytgeotools>ytgeotools</a>: a newish repositority still in early development to help facilitate using yt with geoscience data.</li><li><a href=https://github.com/yt-project/yt_idv>yt_idv</a>: yt&rsquo;s latest interactive data exploration tool</li></ul><p>Finally, you may prefer the initial draft of this post with less narrative that is <a href=https://hackmd.io/qiNip7JkQvqsbu-XbOI-UA>available here</a>.</p><h2 id=general-workflow-for-seismic-data>general workflow for seismic data</h2><p>So the easiest data to work with initially are <a href=http://ds.iris.edu/ds/products/emc-earthmodels/>IRIS EMC earth models</a>. The main reason for this is that to use yt for volume rendering, the simplest approach is to use an already-gridded product. The following code snippets are intentionally incomplete: they are designed more to offer an overview of how to get started. For more complete examples, check out the links above.</p><p>To get started working with 3d data is to read the model data from native storage (e.g., a netcdf file) into yt. At present, the easiest way to do this is with in-memory dataset created using <code>yt.load_uniform_grid()</code> (<a href=https://yt-project.org/doc/reference/api/yt.loaders.html#yt.loaders.load_uniform_grid>docs link</a>).</p><p>How to do that exactly depends (1) on your data and (2) what you&rsquo;re going to do with the data. The second point relates to a current but important limitation of yt: volume rendering requires cartesian coordinates!</p><h3 id=loading-the-raw-data>Loading the raw data</h3><p>So even though volume rendering requires cartesian coordinates, it&rsquo;s worth mentioning that you <strong>can</strong> load in your raw netcdf data in typical latitude, longitude and radius coordinates with yt. This is nice for making maps! To do this, we load the data as an <code>internal_geographic</code> dataset:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
ds <span style=color:#f92672>=</span> yt<span style=color:#f92672>.</span>load_uniform_grid(data,  
                          sizes, 
                          <span style=color:#ae81ff>1.0</span>, 
                          geometry<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#34;internal_geographic&#34;</span>, dims),
                          bbox<span style=color:#f92672>=</span>bbox)
</code></pre></div><p>where:</p><p><code>data</code>: dictionary of field data arrays read from your netcdf</p><p><code>sizes</code>: the shape of the field data</p><p><code>bbox</code>: the bounding box</p><p><code>dims</code>: the ordering of dimensions in the data arrays</p><p>Since we can&rsquo;t volume render with this type of dataset, I won&rsquo;t go over this much, but you can see <a href=https://nbviewer.jupyter.org/github/chrishavlin/AGU2020/blob/main/notebooks/seismic.ipynb#Fixed-Depth-Maps>this notebook</a> for a full example or see <a href=https://nbviewer.jupyter.org/github/chrishavlin/yt_scratch/blob/master/notebooks/yt_obspy_raypath_sampling.ipynb>this notebook</a> for an fun example using obspy and yt together to sample along seismic raypaths.</p><p>At present, the main caveats of this method are:</p><ul><li>can only create fixed-depth maps (no cross-sections)</li><li>still a little buggy (global models seem to work better than non-global models)</li><li>no volume rendering</li></ul><h2 id=geo-volume-rendering-with-yt>geo-volume rendering with yt</h2><p>Now we get to it! Ok, so the first condition, mentioned above is that <strong>data must be in cartesian coordinates</strong>. So to do that, we have to interpolate the seismic model from geographic coordinates to a (uniform) cartesian grid.</p><p>Overall, my approach (in pseudo-python-code) is as follows:</p><p><strong>1. convert</strong> the model&rsquo;s lat, lon, depth arrays to geocentric cartesian coordinates:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lat, lon, radius <span style=color:#f92672>=</span> read_from_raw_data()
x, y, z <span style=color:#f92672>=</span> to_cartesian(lat, lon, radius)
</code></pre></div><p>once we have the raw grid points in cartesian, we</p><p><strong>2. find the bounding box</strong> in cartesian coordinates:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cart_bbox <span style=color:#f92672>=</span> [
             [x<span style=color:#f92672>.</span>min(), x<span style=color:#f92672>.</span>max()], 
             [y<span style=color:#f92672>.</span>min(), y<span style=color:#f92672>.</span>max()],
             [z<span style=color:#f92672>.</span>min(), z<span style=color:#f92672>.</span>max()],
            ]
</code></pre></div><p>If your latitude-longitude-radius ranges are small enough, you might be OK with stopping here and just treating your initial grid as uniform in cartesian. But in most cases if you do that you&rsquo;ll get some unrealistic stretching (and it won&rsquo;t work <strong>at all</strong> for global models), so we need to move on to interpolating.</p><p><strong>3. Create a uniform grid</strong> to cover bounding box:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>x_i <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min_x, max_x, n_x) 
y_i <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min_y, max_y, n_y) 
z_i <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min_z, max_z, n_z) 
</code></pre></div><p><strong>4. find the data values at all those points</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>data_on_cart_grid <span style=color:#f92672>=</span> new array
<span style=color:#66d9ef>for</span> xv, yv, zv <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>meshgrid(x_i, y_i, z_i):
    data_on_cart_grid[xvi, yvi, zvi] <span style=color:#f92672>=</span> sample_the_data(xv, yv, zv)
</code></pre></div><p>This step is the most critical. Its the step that is most prone to computational limitations (the above loop is actually not a great idea, it will be very slow for reasonable <code>n_x</code>, <code>n_y</code> and <code>n_z</code>).</p><p><strong>How to <code>sample_the_data</code> ?</strong></p><p>So the approach I&rsquo;ve taken is to use a KDtree for resampling. The first step here is to create a KDtree with the original cartesian coordinates:</p><pre><code>tree = KDtree(x, y, z)
</code></pre><p>Then, for every point on the new cartesian grid, I use the KDtree to find the <code>N</code> closest points within some max distance, which are then averaged using inverse-distance weighting (IDW). This is nice because it&rsquo;s pretty fast, but all of the extra tuneable parameters can be tricky to deal with and may introduce re-sampling errors. Depending on the resolution of the covering grid relative to the underlying data and how many missing values the underlying data contains, there can often be clear artifacts or just missing data entirely. So this is definitely a step to experiment with further as you start to generate images.</p><p><strong>Once data is sampled</strong></p><p>Once the data is interpolated to the new uniform cartesian grid, we can use <code>yt.load_uniform_grid</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>data <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;dvs&#34;</span>: dvs_on_cartesian}
bbox <span style=color:#f92672>=</span> cart_bbox
sizes <span style=color:#f92672>=</span> dvs_on_cartesian<span style=color:#f92672>.</span>shape
ds <span style=color:#f92672>=</span> yt<span style=color:#f92672>.</span>load_uniform_grid(data, sizes, <span style=color:#ae81ff>1.0</span>)
</code></pre></div><p>And now use volume rendering!</p><p>The above workflow is the basis for my work presented at <a href=https://nbviewer.jupyter.org/github/chrishavlin/AGU2020/blob/main/notebooks/seismic.ipynb#Volume-Rendering>AGU2020</a>
and <a href=https://github.com/earthcube2020/ec20_havlin_etal>EarthCube2020</a>, both of which rely on an initial helper package called <a href=https://github.com/chrishavlin/yt_velmodel_vis>yt_velmodel_vis</a>. While that package should still work, I&rsquo;ve moved development over to a new package called ytgeotools.</p><h3 id=geo-volume-rendering-in-practice>geo-volume rendering in practice:</h3><p>The examples here rely on the new <a href=https://github.com/chrishavlin/ytgeotools>ytgeotools package</a> which helps to streamline the above process of initially loading data into yt while adding some additional analysis functionality. The basic usage for loading an IRIS netcdf file into a yt dataset is:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> yt
<span style=color:#f92672>from</span> ytgeotools.seismology.datasets <span style=color:#f92672>import</span> XarrayGeoSpherical
filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;IRIS/GYPSUM_percent.nc&#34;</span>

<span style=color:#75715e># to get a yt dataset for maps:</span>
ds_yt <span style=color:#f92672>=</span> XarrayGeoSpherical(filename)<span style=color:#f92672>.</span>load_uniform_grid()

<span style=color:#75715e># to get a yt dataset on interpolated cartesian grid:</span>
ds <span style=color:#f92672>=</span> XarrayGeoSpherical(filename)
ds_yt_i <span style=color:#f92672>=</span> ds<span style=color:#f92672>.</span>interpolate_to_uniform_cartesian(
    [<span style=color:#e6db74>&#34;dvs&#34;</span>],
    N<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    max_dist<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    return_yt<span style=color:#f92672>=</span>True,    
)
</code></pre></div><p><strong>should</strong> be useable for any IRIS EMC netcdf file to get a yt dataset.</p><p>The work is still in an early stage and installation is not yet streamlined, so you need to install two packages from source in the following order:</p><ol><li><a href=https://github.com/yt-project/yt_idv>https://github.com/yt-project/yt_idv</a></li><li><a href=https://github.com/chrishavlin/ytgeotools>https://github.com/chrishavlin/ytgeotools</a></li></ol><p>To use any of the mapping functionality, you also need <code>cartopy</code> so you may want to use a conda environment to facilitate that installation.</p><p>So to get started with ytgeotools, first download or clone the two packages above, make sure <code>conda</code> environment is active, then <code>cd</code> into <code>yt_idv</code> and install from source with</p><pre><code>$ python pip install . 
</code></pre><p>Repeat that for <code>ytgeotools</code>.</p><p>Test installation: from a python shell, try:</p><pre><code>&gt;&gt;&gt; import yt_idv
&gt;&gt;&gt; import ytgeotools
&gt;&gt;&gt; import yt
</code></pre><p>There may be some more missing dependencies to install&mldr; hopefully this will be further streamlined soon!</p><h4 id=manual-volume-rendering>manual volume rendering</h4><p>The AGU and EarthCube notebooks use the standard yt volume rendering interfaces. Excerpted from those notebooks, the typical workflow to create an image using ytgeotools would look like:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#f92672>import</span> yt
<span style=color:#f92672>from</span> ytgeotools.seismology.datasets <span style=color:#f92672>import</span> XarrayGeoSpherical

<span style=color:#75715e># load the cartesian dataset</span>
ds_raw <span style=color:#f92672>=</span> XarrayGeoSpherical(filename)
ds <span style=color:#f92672>=</span> ds_raw<span style=color:#f92672>.</span>interpolate_to_uniform_cartesian(
    [<span style=color:#e6db74>&#34;dvs&#34;</span>],
    N<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    max_dist<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    return_yt<span style=color:#f92672>=</span>True,    
)

<span style=color:#75715e># create the scene (loads full dataset into the scene for rendering)</span>
sc <span style=color:#f92672>=</span> yt<span style=color:#f92672>.</span>create_scene(ds, <span style=color:#e6db74>&#34;dvs&#34;</span>) 

<span style=color:#75715e># adjust camera position and orientation (so &#34;up&#34; is surface)</span>
x_c<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>mean(bbox[<span style=color:#ae81ff>0</span>])  
y_c<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>mean(bbox[<span style=color:#ae81ff>1</span>])
z_c<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>mean(bbox[<span style=color:#ae81ff>2</span>])
center_vec <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([x_c,y_c,z_c])
center_vec <span style=color:#f92672>=</span> center_vec <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(center_vec)
pos<span style=color:#f92672>=</span>sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>position
sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>set_position(pos,north_vector<span style=color:#f92672>=</span>center_vec)

<span style=color:#75715e># adjust camera zoom </span>
zoom_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span> <span style=color:#75715e># &lt; 1 zooms in</span>
init_width<span style=color:#f92672>=</span>sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>width
sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>width <span style=color:#f92672>=</span> (init_width <span style=color:#f92672>*</span> zoom_factor)

<span style=color:#75715e># set transfer function</span>
<span style=color:#75715e># initialize the tf object by setting the data bounds to consider</span>
dvs_min<span style=color:#f92672>=-</span><span style=color:#ae81ff>8</span>
dvs_max<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span> 
tf <span style=color:#f92672>=</span> yt<span style=color:#f92672>.</span>ColorTransferFunction((dvs_min,dvs_max))

<span style=color:#75715e># set gaussians to add </span>
TF_gaussians<span style=color:#f92672>=</span>[
    {<span style=color:#e6db74>&#39;center&#39;</span>:<span style=color:#f92672>-.</span><span style=color:#ae81ff>8</span>,<span style=color:#e6db74>&#39;width&#39;</span>:<span style=color:#f92672>.</span><span style=color:#ae81ff>1</span>,<span style=color:#e6db74>&#39;RGBa&#39;</span>:(<span style=color:#ae81ff>1.</span>,<span style=color:#ae81ff>0.</span>,<span style=color:#ae81ff>0.</span>,<span style=color:#f92672>.</span><span style=color:#ae81ff>5</span>)},
    {<span style=color:#e6db74>&#39;center&#39;</span>:<span style=color:#f92672>.</span><span style=color:#ae81ff>5</span>,<span style=color:#e6db74>&#39;width&#39;</span>:<span style=color:#f92672>.</span><span style=color:#ae81ff>2</span>,<span style=color:#e6db74>&#39;RGBa&#39;</span>:(<span style=color:#ae81ff>0.1</span>,<span style=color:#ae81ff>0.1</span>,<span style=color:#ae81ff>1.</span>,<span style=color:#f92672>.</span><span style=color:#ae81ff>8</span>)}
]

<span style=color:#66d9ef>for</span> gau <span style=color:#f92672>in</span> TF_gaussians:
    tf<span style=color:#f92672>.</span>add_gaussian(gau[<span style=color:#e6db74>&#39;center&#39;</span>],gau[<span style=color:#e6db74>&#39;width&#39;</span>],gau[<span style=color:#e6db74>&#39;RGBa&#39;</span>])
    
source <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>sources[<span style=color:#e6db74>&#39;source_00&#39;</span>]
source<span style=color:#f92672>.</span>set_transfer_function(tf)

<span style=color:#75715e># adjust resolution of rendering </span>
res <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>get_resolution()  
res_factor <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
new_res <span style=color:#f92672>=</span> (int(res[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>*</span>res_factor),int(res[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span>res_factor))
sc<span style=color:#f92672>.</span>camera<span style=color:#f92672>.</span>set_resolution(new_res)

<span style=color:#75715e># if in a notebook</span>
sc<span style=color:#f92672>.</span>show()

<span style=color:#75715e># if in a script</span>
sc<span style=color:#f92672>.</span>save()
</code></pre></div><p>As mentioned above, there can be a lot of trial and error for transfer functions and also camera positioning. And making videos requires adjusting the camera position, saving off the images and then stitching together the frames with a different tool.</p><h3 id=interactive-volume-rendering>interactive volume rendering</h3><p>ytgeotools is built to easily interface with yt&rsquo;s latest interactive rendering environment, <a href=https://github.com/yt-project/yt_idv>yt_idv</a>. It&rsquo;s great for getting a sense of whether or not the interpolation is working. At present, the transfer function behavior is slightly different: the custom transfer function functionality is not fully working yet, but the maximum value and projection functions work! The following script loads a dataset and spawns a rendering environment:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> yt_idv
<span style=color:#f92672>from</span> ytgeotools.seismology.datasets <span style=color:#f92672>import</span> XarrayGeoSpherical


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>refill</span>(vals):
    <span style=color:#75715e># vals will be a numpy array containing the data on the </span>
    <span style=color:#75715e># the interpolated grid. We can modify/mask it any way </span>
    <span style=color:#75715e># we want here and return it.</span>
    vals[np<span style=color:#f92672>.</span>isnan(vals)] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
    vals[vals <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
    <span style=color:#66d9ef>return</span> vals


filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;IRIS/NWUS11-S_percent.nc&#34;</span>
ds <span style=color:#f92672>=</span> XarrayGeoSpherical(filename)
ds_yt <span style=color:#f92672>=</span> ds<span style=color:#f92672>.</span>interpolate_to_uniform_cartesian(
    [<span style=color:#e6db74>&#34;dvs&#34;</span>],
    N<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    max_dist<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
    return_yt<span style=color:#f92672>=</span>True,
    rescale_coords<span style=color:#f92672>=</span>True, <span style=color:#75715e>## IMPORTANT to rescale!</span>
    apply_functions<span style=color:#f92672>=</span>[refill, np<span style=color:#f92672>.</span>abs],
)

rc <span style=color:#f92672>=</span> yt_idv<span style=color:#f92672>.</span>render_context(height<span style=color:#f92672>=</span><span style=color:#ae81ff>800</span>, width<span style=color:#f92672>=</span><span style=color:#ae81ff>800</span>, gui<span style=color:#f92672>=</span>True)
sg <span style=color:#f92672>=</span> rc<span style=color:#f92672>.</span>add_scene(ds_yt, <span style=color:#e6db74>&#34;dvs&#34;</span>, no_ghost<span style=color:#f92672>=</span>True)
rc<span style=color:#f92672>.</span>run()
</code></pre></div><p>For initially loading in seismic data that are velocity anomalies, I often find it helpful to mask out positive and negative anomalies and investigate them separately. So in the <code>interpolate_to_uniform_cartesian</code> method above I included the option to pass in function handles. These functions will get run after the re-gridding and must accept and return a single numpy array.</p><h3 id=annotations>annotations:</h3><p>Finally, it&rsquo;s important to be able to add georeferenced annotations to the renderings in order to orient oneself. When using yt&rsquo;s manual plotting interface we can do that with Point and Line sources:</p><pre><code class="language-python=" data-lang="python=">from yt.visualization.volume_rendering.api import LineSource, PointSource
</code></pre><p>The EarthCube/AGU notebooks have some helper functions to streamline this, so check those out for examples. One important note is that adding the Point and Line sources can affect the overall brightness of the scene, which demands some trial and error to adjust the opacity of the sources so that they do not wash out the volume rendering.</p><p>Recent work in yt_idv added the ability to add arbitrary curves, so ytgeotools will soon have some extra functionality to make it easier to add georeferenced data to the interactive plots as well.</p><h2 id=summary>summary</h2><p>This post was meant as gentle introduction to volume rendering in yt. I hope you found it useful! If you&rsquo;re interested in helping to develop <code>ytgeotools</code>, contributions via github pull request are welcome!</p></div><div class=share-buttons><a class=twitter-share-button href=# title="Share on Twitter" data-url=https://chrishavlin.github.io/post/ytgeosummary/ data-text="A quick primer on volume rendering of geophysics data with yt"><i class="fab fa-twitter"></i></a><a class=linkedin-share-button href=# title="Share on LinkedIn" data-url=https://chrishavlin.github.io/post/ytgeosummary/ data-text="A quick primer on volume rendering of geophysics data with yt"><i class="fab fa-linkedin-in"></i></a><a class=facebook-share-button href=# title="Share on Facebook" data-url=https://chrishavlin.github.io/post/ytgeosummary/ data-text="A quick primer on volume rendering of geophysics data with yt"><i class="fab fa-facebook"></i></a><a class=telegram-share-button href=# title="Share on Telegram" data-url=https://chrishavlin.github.io/post/ytgeosummary/ data-text="A quick primer on volume rendering of geophysics data with yt"><i class="fab fa-telegram"></i></a><a class=pinterest-share-button href=# title="Share on Pinterest" data-url=https://chrishavlin.github.io/post/ytgeosummary/ data-text="A quick primer on volume rendering of geophysics data with yt"><i class="fab fa-pinterest"></i></a></div></div></main><footer><div><p>&copy; Chris Havlin 2023
&#183; <a href=https://creativecommons.org/licenses/by-sa/4.0 target=_blank>CC BY-SA 4.0</a>
&#183; Build with <a href=https://gohugo.io/ target=_blank>Hugo</a> & <a href=https://themes.gohugo.io/soho/ target=_blank>Soho</a> theme</p></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0=" crossorigin=anonymous></script><script src=/js/jquery.min.js></script><script src=/js/soho.js></script></body></html>